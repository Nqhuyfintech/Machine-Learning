{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "\n",
    "# Dòng này sẽ khởi tạo một danh sách rỗng để lưu trữ các liên kết\n",
    "arr = []\n",
    "\n",
    "# Vòng lặp để lặp qua các trang từ 0 đến 19\n",
    "for i in range(20):\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "    # Đăng nhập trang web\n",
    "    driver.get(f'https://www.nhatot.com/mua-ban-bat-dong-san-tp-ho-chi-minh?page={i}')\n",
    "    \n",
    "    # Sử dụng thuộc tính class_name để tìm kiếm phần tử\n",
    "    elements = driver.find_elements(By.CLASS_NAME, 'AdItem_adItem__gDDQT')\n",
    "    \n",
    "    # Lặp qua từng phần tử và thêm liên kết vào danh sách\n",
    "    for element in elements:\n",
    "        arr.append(element.get_attribute('href'))\n",
    "    driver.quit()\n",
    "\n",
    "# Thực hiện các hành động trên phần tử\n",
    "# Ví dụ: in ra văn bản của nó\n",
    "for link in arr:\n",
    "    print(link)\n",
    "\n",
    "# Đóng trình duyệt\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "b2f108504754d641"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LẤY NHIỀU TRANG CÙNG LÚC"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a170655bc170c227"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception occurred on page 6: NoSuchWindowException\n",
      "Exception occurred on page 8: NoSuchWindowException\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Dòng này sẽ khởi tạo một danh sách rỗng để lưu trữ các liên kết\n",
    "arr = []\n",
    "\n",
    "# Định nghĩa hàm để lấy dữ liệu từ mỗi trang\n",
    "def get_links_from_page(page_number):\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "    driver.get(f'https://www.nhatot.com/mua-ban-bat-dong-san-tp-ho-chi-minh?page={page_number}')\n",
    "    \n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    try:\n",
    "        elements = wait.until(EC.presence_of_all_elements_located((By.CLASS_NAME, 'AdItem_adItem__gDDQT')))\n",
    "        links = [element.get_attribute('href') for element in elements]\n",
    "    except Exception as e:\n",
    "        print(f\"Exception occurred on page {page_number}: {type(e).__name__}\")\n",
    "        links = []\n",
    "    \n",
    "    driver.quit()\n",
    "    \n",
    "    return links\n",
    "\n",
    "# Sử dụng ThreadPoolExecutor để chạy các tác vụ đồng thời\n",
    "with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    futures = {executor.submit(get_links_from_page, i): i for i in range(1000)}\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        page_number = futures[future]\n",
    "        try:\n",
    "            links = future.result()\n",
    "            arr.extend(links)\n",
    "            print(f\"Page {page_number} completed.\")\n",
    "        except Exception as exc:\n",
    "            print(f\"{type(exc).__name__} occurred during fetching page {page_number}.\")\n",
    "        else:\n",
    "            print(f\"Finished fetching links from page {page_number}\")\n",
    "\n",
    "# In ra tất cả các liên kết đã thu thập\n",
    "for link in arr:\n",
    "    print(link)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "3aba5d2edffebc4b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.DataFrame(arr,columns=['links'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c3a33882e43dc5e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.to_csv('chotot.csv',index=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "228772c1eae751ce"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# VÌ DATA QUÁ LỚN NÊN CHIA THÀNH NHIỀU BỘ MỖI BỘ 1000 LINK"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1ab2f3edcef0b21f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Đọc dữ liệu từ tập tin CSV\n",
    "df = pd.read_csv('chotot.csv')\n",
    "\n",
    "# Chia dữ liệu thành các file nhỏ\n",
    "chunk_size = 1000\n",
    "for i in range(0, len(df), chunk_size):\n",
    "    chunk = df.iloc[i:i+chunk_size]\n",
    "    chunk.to_csv(f'chotot_{i//chunk_size+1}.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f0771815be226fc8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CHẠY TỪNG FILE"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "22583a8eaa411132"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Load the links from the CSV file\n",
    "df = pd.read_csv('chotot_14.csv')\n",
    "links = df['links'].tolist()  # Assuming 'links' is the column name in your CSV\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b57ca9d67d47776d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LẤY DATA"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "40be3232e2a0298a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "\n",
    "# Initialize an empty list to store the detailed property information\n",
    "detailed_property_info = []\n",
    "\n",
    "# Function to extract the address, price, size, and other details from a webpage\n",
    "def extract_detailed_property_info(link):\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "    try:\n",
    "        driver.get(link)\n",
    "        \n",
    "        # Your existing logic to extract details goes here...\n",
    "        try:\n",
    "            address_element = driver.find_element(By.XPATH, \"//span[@class='fz13']\")\n",
    "            address_text = address_element.text\n",
    "        except Exception:\n",
    "            address_text = None\n",
    "        \n",
    "        # Extract the size, assuming it's still found with the same XPath\n",
    "        try:\n",
    "            size_element = driver.find_element(By.XPATH, \"//span[@itemprop='size']\")\n",
    "            size_text = size_element.text\n",
    "        except Exception:\n",
    "            size_text = None\n",
    "        \n",
    "        # Extract additional details\n",
    "        rooms_text = None\n",
    "        toilets_text = None\n",
    "        property_legal_document_text = None\n",
    "        furnishing_sell_text = None\n",
    "        length_text = None\n",
    "        price_m2_text = None\n",
    "        direction_text = None\n",
    "        floors_text = None\n",
    "        house_type_text = None\n",
    "        width_text = None\n",
    "        \n",
    "        try:\n",
    "            rooms_element = driver.find_element(By.XPATH, \"//span[@itemprop='rooms']\")\n",
    "            rooms_text = rooms_element.text\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            toilets_element = driver.find_element(By.XPATH, \"//span[@itemprop='toilets']\")\n",
    "            toilets_text = toilets_element.text\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            property_legal_document_element = driver.find_element(By.XPATH, \"//span[@itemprop='property_legal_document']\")\n",
    "            property_legal_document_text = property_legal_document_element.text\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            furnishing_sell_element = driver.find_element(By.XPATH, \"//span[@itemprop='furnishing_sell']\")\n",
    "            furnishing_sell_text = furnishing_sell_element.text\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            length_element = driver.find_element(By.XPATH, \"//span[@itemprop='length']\")\n",
    "            length_text = length_element.text\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            price_m2_element = driver.find_element(By.XPATH, \"//span[@itemprop='price_m2']\")\n",
    "            price_m2_text = price_m2_element.text\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            direction_element = driver.find_element(By.XPATH, \"//span[@itemprop='direction']\")\n",
    "            direction_text = direction_element.text\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            floors_element = driver.find_element(By.XPATH, \"//span[@itemprop='floors']\")\n",
    "            floors_text = floors_element.text\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            house_type_element = driver.find_element(By.XPATH, \"//span[@itemprop='house_type']\")\n",
    "            house_type_text = house_type_element.text\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            width_element = driver.find_element(By.XPATH, \"//span[@itemprop='width']\")\n",
    "            width_text = width_element.text\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "        # Return the extracted information\n",
    "        return {\n",
    "            'Address': address_text,\n",
    "            'Size': size_text,\n",
    "            'Rooms': rooms_text,\n",
    "            'Toilets': toilets_text,\n",
    "            'Legal Document': property_legal_document_text,\n",
    "            'Furnishing Sell': furnishing_sell_text,\n",
    "            'Length': length_text,\n",
    "            'Width': width_text,\n",
    "            'Direction': direction_text,\n",
    "            'Floors': floors_text,\n",
    "            'House Type': house_type_text,\n",
    "            'Price': price_m2_text\n",
    "        }\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "# Assuming 'links' is a list of URLs you want to scrape\n",
    "# Use ThreadPoolExecutor to parallelize the scraping process\n",
    "with concurrent.futures.ThreadPoolExecutor(10) as executor:\n",
    "    future_to_link = {executor.submit(extract_detailed_property_info, link): link for link in links}\n",
    "    \n",
    "    for future in concurrent.futures.as_completed(future_to_link):\n",
    "        link = future_to_link[future]\n",
    "        try:\n",
    "            data = future.result()\n",
    "        except Exception as exc:\n",
    "            print('%r generated an exception: %s' % (link, exc))\n",
    "        else:\n",
    "            detailed_property_info.append(data)\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "detailed_property_info_df = pd.DataFrame(detailed_property_info)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(detailed_property_info_df)\n",
    "detailed_property_info_df.to_excel('data14.xlsx', index=True)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d897a0b6cf734ea4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Gộp CÁC FILE"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8c6f6b029634f147"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Danh sách các file Excel cần hợp nhất\n",
    "files = ['data1.xlsx', 'data2.xlsx', 'data3.xlsx','data4.xlsx', 'data5.xlsx', 'data6.xlsx','data7.xlsx', 'data8.xlsx', 'data9.xlsx','data10.xlsx', 'data11.xlsx', 'data13.xlsx','data14.xlsx', 'data15.xlsx','data16.xlsx', 'data17.xlsx', 'data18.xlsx',]\n",
    "\n",
    "# Khởi tạo một DataFrame rỗng\n",
    "merged_df = pd.DataFrame()\n",
    "\n",
    "# Lặp qua từng file và nối vào DataFrame\n",
    "for file in files:\n",
    "    df = pd.read_excel(file)\n",
    "    merged_df = pd.concat([merged_df, df], ignore_index=True)\n",
    "\n",
    "# Lưu DataFrame hợp nhất vào một file Excel mới\n",
    "merged_df.to_excel('datachotot.xlsx', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-25T03:24:24.788480100Z",
     "start_time": "2024-05-25T03:24:18.660482400Z"
    }
   },
   "id": "688286b5d28bdd00"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
